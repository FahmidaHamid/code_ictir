#from __future__ import division
import os
import sys
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import sent_tokenize
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus.reader.plaintext import PlaintextCorpusReader
from nltk.stem.porter import *
from nltk.stem.snowball import SnowballStemmer
import nltk.tag
import string
import networkx as nx
from collections import defaultdict
import re
import math
import unicodedata
import numpy

class Sample:
  name = ''
  lemma_names = set()
  

class SentenceSample:
    ssen = ''
    weight = 0.000
    senIndex = 0
    
class overlapped :
    sen1 = ''
    sen2 = ''
    overLap = 0


class TextRank:
    
    def __init__(self, pdfFile, fullFilePath, readPath, absPath, kePath, writePath):
        self.engStopWords = set(stopwords.words('english'))
        self.excludeSet = set(string.punctuation).union(self.engStopWords)
        self.lmtzr = WordNetLemmatizer()

        self.pdfP = pdfPath
        self.fileP = fullFilePath
        self.readP = readPath
        self.abstractP = absPath
        self.keP = kePath  
        self.writeP = writePath 


    def findSimilarity(self, x,y):
    
    
        if(len(x) == 0 or len(y) == 0) :
           return 0
        else :
           z = x.intersection(y)
           weight = len(z)/ (len(x) + len(y) - len(z))
           return weight


    def get_wordnet_pos(self, treebank_tag):
    
        
        if treebank_tag.startswith('N'):
           return wn.NOUN
        elif treebank_tag.startswith('J'):
           return wn.ADJ
        elif treebank_tag.startswith('R'):
           return wn.ADV
        else:
           return ''

    def buildGraph(self, sentences):

        #extra_abbreviations = ['dr', 'vs', 'mr', 'mrs','ms', 'prof', 'inc', 'i.e', 'eg', 'al', 'fig']
        #sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
        #sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)              

        g = nx.DiGraph()
        wordList = defaultdict(set)
        for sid, s in enumerate(sentences):
            print(sid, '>>', s)
            ids = set()
            s = s.lower()
            #tokens = nltk.word_from nltk.stem.porter import *tokenize(s.translate(self.tbl))
            tokens = nltk.word_tokenize(s.translate(string.punctuation))   
            tags = nltk.pos_tag(tokens)
            print(tags)
            wid = 0
            for ws in tags:
                z = ws[0].rstrip('\'\"-,.:;!?()[]{}')
                z = z.lstrip('\'\"-,.:;!?()[]{}') 
                if len(z) > 0:  
                   if ws[0] not in self.engStopWords:
                     w = z.lower()
                     pos = ws[1]
                     poswn = self.get_wordnet_pos(pos)
                     if poswn: #do not accept anything otherthan nouns
                        
                       myWord = self.lmtzr.lemmatize(w, poswn)
                       wsynset =  wn.synsets(myWord, poswn)
                       #print('word: ', myWord)
                       
                       s1 = Sample()
                       word_id = str(wid) + '#'+str(sid)
                       s1.name = str(myWord)
                       if len(wsynset) > 0 :
                           wlemmas = wsynset[0].lemmas()
                           for wl in wlemmas:
                               s1.lemma_names.add(str(wl.name()))
                           #print(s1.lemma_names)
                       if s1.name not in wordList:
                            wordList[s1.name] = s1.lemma_names	#global
                       ids.add((word_id,s1)) #local --> for each sentence
                       g.add_node(s1.name)
                       wid += 1
        
            windowRange = 4
            for x in ids :
                for y in ids :
                   if x[0] != y[0] : # not the same word
                      idx = x[0]
                      idy = y[0]
                      partx = x[0].split('#')
                      party = y[0].split('#')
                      if abs(int(partx[0]) - int(party[0])) < windowRange :
                        g.add_edge(x[1].name,y[1].name, weight = 0.01)
                        g.add_edge(y[1].name, x[1].name, weight = 0.01)
                        
        sz = g.number_of_edges()
        if sz == 0:
             zs = 0
        else:   
             zs = float(1.0/float(sz))
        wordConsidered = set()
        for v1 in wordList.keys() :
           for v2 in wordList.keys() :
                if v1 != v2:
                    set1 = wordList[v1]
                    set2 = wordList[v2]
                    pair = (v1,v2)
                    pairr = (v2,v1)
                    if (pair not in wordConsidered) :
                        wordConsidered.add(pair)
                        wordConsidered.add(pairr)
                        similarity = self.findSimilarity(set1,set2)
                        if similarity > 0.000 :
                            if g.has_edge(v1,v2) :
                                  g.edge[v1][v2]['weight'] += zs * similarity
                                  g.edge[v2][v1]['weight'] += zs * similarity
                            else :
                                  g.add_edge(v1,v2, weight = zs * similarity)
                                  g.add_edge(v2, v1, weight = zs * similarity)
    
    
        #print(wordList)
        print(len(wordList))
        print(g.number_of_nodes())   
        return (g, len(wordList))


    def applyTextRank(self, g):
    
        alpha = float(0.85)
        pr = nx.pagerank_scipy(g, alpha=0.85, max_iter=100000, weight = 'weight')
        return pr



    def constructSentences(self, sentences, pg, limit):
                  
        sentenceList = []
        #words = sorted(pg.items(), key= lambda x: x[1], reverse=True)
        totalWeight = 0.00
        for w in pg:
            totalWeight += pg[w]
        g_nodes = len(pg.keys())
        #print(' Total Weight:: ', totalWeight)
        #print(' Total Nodes:: ', g_nodes)  
       
        for sindex, s in enumerate(sentences) :
            xs = SentenceSample()
            xs.ssen = s
            xs.senIndex = sindex
            
            s_weight = 0.00
            s_nodes = 0
            s =s.lower()
            tokens = nltk.word_tokenize(s.translate(string.punctuation))   
            
            for n in tokens :
                z = n.rstrip('\'\"-,.:;!?()[]{}')
                z = z.lstrip('\'\"-,.:;!?()[]{}')    
                if z in pg.keys() :
                  s_weight += math.fabs(pg[z])
                  s_nodes += 1
            if s_nodes > 0 and s_weight > 0.00 :
               # xs.matchId = (s_weight * float(g_nodes)) / ( float(s_nodes) * totalWeight)
                xs.matchId = s_weight / float(s_nodes)
            else :
                xs.matchId = 0.00
            
            sentenceList.append(xs)
         
        
        sentenceList = sorted(sentenceList, key=lambda ps1: ps1.matchId, reverse = True)
        topSentences = sentenceList[:limit]
        topSentences = sorted(topSentences, key = lambda ps1: ps1.senIndex, reverse = False)
        ss = ''
        for t in topSentences:
             ss = ss + ' ' + t.ssen.lower() 
        return (topSentences, ss)



    def getLemmas(self, setA):

        setB = set()
        stemmer = SnowballStemmer("english")
        for a in setA:
            setB.add(stemmer.stem(a))
        return setB          
    

    def compareAbstract(self, summary_sentences, abs_sentences, n, fname):

         precision = 0
         recall = 0
         avgO = 0
         i = 0
         i_measure = 0 
         print('Abstract of ', fname) 
         tokens = set(nltk.word_tokenize(abs_sentences.translate(string.punctuation)))   
         tokens = tokens.difference(self.excludeSet)
         atokens = self.getLemmas(tokens)
         print(atokens)   
         k = len(atokens)  
         trTokens = set(nltk.word_tokenize(summary_sentences.translate(string.punctuation)))
         trTokens = trTokens.difference(self.excludeSet)
         atrTokens = self.getLemmas(trTokens)     
         print(atrTokens)
         l = len(atrTokens)
         AB = atokens.intersection(atrTokens)
         print(AB) 
         i = len(AB)
         if n > 0: 
           precision = float(i)/float(l)
           recall = float(i)/float(k)
           avgO = float(float(k * l) / float( n ))
           i_measure = float(i)/ avgO
           print('P: ', precision, ' R: ', recall, ' i_measure: ', i_measure) 
         return (precision,recall,avgO,i, i_measure)
              
                  
                  
    def writeSummary(self, dirName, summary_sentences, pr, fname):
        
        dir = os.path.dirname(self.writeP + '/'+ i)

        try:
          os.stat(dir)
        except:
          os.mkdir(dir)       

          
        rem = open(dir + '/' + fname, 'w')
        for s in summary_sentences:
            rem.write(s.ssen+'\n')
        
        # words = sorted(pr.items(), key = lambda x: x[1], reverse = True)       
        # for w in words:
        #     rem.write(w[0]+ '>>'+str(w[1]) + '\n')
        # rem.write('Total: '+ str(len(words)))
        
          
        rem.close()
       
         
                  
                  

    def summarize(self, dirName, text, fname, sum_size, abslines):
            myStats = dict()
            sentences = nltk.sent_tokenize(text)
            (g,n) = self.buildGraph(sentences)
            pg = self.applyTextRank(g)
            (summary_sentences, ss) = self.constructSentences(sentences,pg,sum_size)
            self.writeSummary(dirName,summary_sentences,pg, fname)
            (p,r,avg0,i, i_measure) = self.compareAbstract(ss, abslines, n, fname)
            return (p,r,avg0,i,i_measure)   
       

